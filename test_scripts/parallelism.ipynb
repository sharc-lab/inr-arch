{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "if Path.cwd().name == \"test_scripts\":\n",
    "    os.chdir(Path.cwd().parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from typing import Optional\n",
    "import subprocess\n",
    "\n",
    "\n",
    "# from rich import print\n",
    "from rich.pretty import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.analysis.parallelism_v4 import update_graph\n",
    "from src.analysis.latency.engine import DFG, StreamIOType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiGraph with 108 nodes and 131 edges\n"
     ]
    }
   ],
   "source": [
    "model_fp = Path(\"./src/test_files_256/new_grad_lastdim_small.pkl\")\n",
    "\n",
    "with open(model_fp, \"rb\") as f:\n",
    "    G_data = pickle.load(f)\n",
    "\n",
    "print(G_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating nodes: 100%|██████████| 108/108 [00:51<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached DFG from /export/hdd/scratch/rsarkar30/INR/inr-dsp/src/test_files_256/new_grad_lastdim_small_dfg.pkl\n"
     ]
    }
   ],
   "source": [
    "model_dfg_fp = model_fp.with_stem(f\"{model_fp.stem}_dfg\")\n",
    "try:\n",
    "    with open(model_dfg_fp, \"rb\") as f:\n",
    "        print(f\"Loading cached DFG from {model_dfg_fp.absolute()}\")\n",
    "        dfg: DFG = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    dfg = update_graph(G_data)\n",
    "    with open(model_dfg_fp, \"wb\") as f:\n",
    "        print(f\"Saving DFG to {model_dfg_fp.absolute()}\")\n",
    "        pickle.dump(dfg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not dfg.has_cycle(), \"Original DFG should not have cycles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dfg.with_depths({stream.name: 2 for stream in dfg.nodes.streams}).has_cycle(), \"New DFG should have a cycle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deadlock found involving 76 stream(s) (52.4% of total)\n",
      "    Increased participant depths; new maximum depth: 4\n",
      "Deadlock found involving 76 stream(s) (52.4% of total)\n",
      "    Increased participant depths; new maximum depth: 8\n",
      "Deadlock found involving 76 stream(s) (52.4% of total)\n",
      "    Increased participant depths; new maximum depth: 16\n",
      "Deadlock found involving 76 stream(s) (52.4% of total)\n",
      "    Increased participant depths; new maximum depth: 32\n",
      "Deadlock found involving 76 stream(s) (52.4% of total)\n",
      "    Increased participant depths; new maximum depth: 64\n",
      "Deadlock found involving 76 stream(s) (52.4% of total)\n",
      "    Increased participant depths; new maximum depth: 128\n",
      "Deadlock found involving 76 stream(s) (52.4% of total)\n",
      "    Increased participant depths; new maximum depth: 256\n",
      "Deadlock found involving 76 stream(s) (52.4% of total)\n",
      "    Increased participant depths; new maximum depth: 512\n",
      "Deadlock found involving 76 stream(s) (52.4% of total)\n",
      "    Increased participant depths; new maximum depth: 1024\n",
      "Deadlock found involving 76 stream(s) (52.4% of total)\n",
      "    Increased participant depths; new maximum depth: 2048\n",
      "Deadlock found involving 76 stream(s) (52.4% of total)\n",
      "    Increased participant depths; new maximum depth: 4096\n",
      "Deadlock found involving 76 stream(s) (52.4% of total)\n",
      "    Increased participant depths; new maximum depth: 8192\n",
      "Deadlock found involving 76 stream(s) (52.4% of total)\n",
      "    Increased participant depths; new maximum depth: 16384\n",
      "Deadlock found involving 76 stream(s) (52.4% of total)\n",
      "    Increased participant depths; new maximum depth: 32768\n",
      "Found a configuration with no deadlock!\n",
      "     Maximum depth: 32768\n",
      "     Sum of depths: 2490506\n",
      "    Total increase: 2490216\n"
     ]
    }
   ],
   "source": [
    "IGNORED_STREAM_SUFFIXES = (\"__out_stream\", \"__temp_stream\")\n",
    "depths = {stream.name: 2 for stream in dfg.nodes.streams if not stream.name.endswith(IGNORED_STREAM_SUFFIXES)}\n",
    "while True:\n",
    "    participants =  dfg.with_depths(depths).get_cycle_participants()\n",
    "    if not participants:\n",
    "        break\n",
    "    participants = set(node.stream for node in participants if node.stream.name in depths)\n",
    "    print(f\"Deadlock found involving {len(participants)} stream(s) ({len(participants) / len(depths):.1%} of total)\")\n",
    "    for participant in participants:\n",
    "        depths[participant.name] *= 2\n",
    "    print(\"    Increased participant depths; new maximum depth:\", max(depths.values()))\n",
    "print(\"Found a configuration with no deadlock!\")\n",
    "print(\"     Maximum depth:\", max(depths.values()))\n",
    "print(\"     Sum of depths:\", sum(depths.values()))\n",
    "print(\"    Total increase:\", sum(depths.values()) - (2 * len(depths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9/145) Stream fn_Cos_2__in_0_stream cannot be safely reduced to depth 16384. (0/1 reduced so far)\n",
      "(15/145) Stream fn_Cos_10__in_0_stream cannot be safely reduced to depth 16384. (0/2 reduced so far)\n",
      "(19/145) Stream fn_Mul_23__in_1_stream cannot be safely reduced to depth 16384. (0/3 reduced so far)\n",
      "(23/145) Stream fn_Cos_9__in_0_stream cannot be safely reduced to depth 16384. (0/4 reduced so far)\n",
      "(25/145) Stream fn_Cos_1__in_0_stream cannot be safely reduced to depth 16384. (0/5 reduced so far)\n",
      "(33/145) Stream fn_Mul_9__in_1_stream cannot be safely reduced to depth 16384. (0/6 reduced so far)\n",
      "(37/145) Stream fn_Mul_7__in_1_stream cannot be safely reduced to depth 16384. (0/7 reduced so far)\n",
      "(38/145) Stream fn_Mul_17__in_1_stream cannot be safely reduced to depth 16384. (0/8 reduced so far)\n",
      "(44/145) Stream fn_Mul_25__in_1_stream cannot be safely reduced to depth 16384. (0/9 reduced so far)\n",
      "(55/145) Stream fn_Cos_11__in_0_stream cannot be safely reduced to depth 16384. (0/10 reduced so far)\n",
      "(66/145) Stream fn_Mul_5__in_1_stream cannot be safely reduced to depth 16384. (0/11 reduced so far)\n",
      "(77/145) Stream fn_Cos_6__in_0_stream cannot be safely reduced to depth 16384. (0/12 reduced so far)\n",
      "(85/145) Stream fn_Cos_5__in_0_stream cannot be safely reduced to depth 16384. (0/13 reduced so far)\n",
      "(92/145) Stream fn_Mul_15__in_1_stream cannot be safely reduced to depth 16384. (0/14 reduced so far)\n",
      "(95/145) Stream fn_Mul_21__in_1_stream cannot be safely reduced to depth 16384. (0/15 reduced so far)\n",
      "(103/145) Stream fn_Cos_3__in_0_stream cannot be safely reduced to depth 16384. (0/16 reduced so far)\n",
      "(125/145) Stream fn_Mul_13__in_1_stream cannot be safely reduced to depth 16384. (0/17 reduced so far)\n",
      "(137/145) Stream fn_Cos_7__in_0_stream cannot be safely reduced to depth 16384. (0/18 reduced so far)\n"
     ]
    }
   ],
   "source": [
    "reduced = 0\n",
    "attempted = 0\n",
    "for i, (name, depth) in enumerate(depths.items()):\n",
    "    if depth > 2:\n",
    "        attempted += 1\n",
    "        test_depths = {**depths, name: 2}\n",
    "        if not dfg.with_depths(test_depths).has_cycle():\n",
    "            reduced += 1\n",
    "            print(f\"({i + 1}/{len(depths)}) Stream {name} can be safely reduced to depth 16384. ({reduced}/{attempted} reduced so far)\")\n",
    "            depths = test_depths\n",
    "        else:\n",
    "            print(f\"({i + 1}/{len(depths)}) Stream {name} cannot be safely reduced to depth 16384. ({reduced}/{attempted} reduced so far)\")\n",
    "    # else:\n",
    "    #     print(f\"({i + 1}/{len(depths)}) Stream {name} is already depth <=16384. ({reduced}/{attempted} reduced so far)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fn_Cos_2__in_0_stream': 32768,\n",
       " 'fn_Cos_10__in_0_stream': 32768,\n",
       " 'fn_Mul_23__in_1_stream': 32768,\n",
       " 'fn_Cos_9__in_0_stream': 32768,\n",
       " 'fn_Cos_1__in_0_stream': 32768,\n",
       " 'fn_Mul_9__in_1_stream': 32768,\n",
       " 'fn_Mul_7__in_1_stream': 32768,\n",
       " 'fn_Mul_17__in_1_stream': 32768,\n",
       " 'fn_Mul_25__in_1_stream': 32768,\n",
       " 'fn_Cos_11__in_0_stream': 32768,\n",
       " 'fn_Mul_5__in_1_stream': 32768,\n",
       " 'fn_Cos_6__in_0_stream': 32768,\n",
       " 'fn_Cos_5__in_0_stream': 32768,\n",
       " 'fn_Mul_15__in_1_stream': 32768,\n",
       " 'fn_Mul_21__in_1_stream': 32768,\n",
       " 'fn_Cos_3__in_0_stream': 32768,\n",
       " 'fn_Mul_13__in_1_stream': 32768,\n",
       " 'fn_Cos_7__in_0_stream': 32768}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in depths.items() if v > 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_so_far = dfg.with_depths({'fn_Cos_2__in_0_stream': 32768,\n",
    " 'fn_Cos_10__in_0_stream': 32768,\n",
    " 'fn_Mul_23__in_1_stream': 32768,\n",
    " 'fn_Cos_9__in_0_stream': 32768,\n",
    " 'fn_Cos_1__in_0_stream': 32768,\n",
    " 'fn_Mul_9__in_1_stream': 32768,\n",
    " 'fn_Mul_7__in_1_stream': 32768,\n",
    " 'fn_Mul_17__in_1_stream': 32768,\n",
    " 'fn_Mul_25__in_1_stream': 32768,\n",
    " 'fn_Cos_11__in_0_stream': 32768,\n",
    " 'fn_Mul_5__in_1_stream': 32768,\n",
    " 'fn_Cos_6__in_0_stream': 32768,\n",
    " 'fn_Cos_5__in_0_stream': 32768,\n",
    " 'fn_Mul_15__in_1_stream': 32768,\n",
    " 'fn_Mul_21__in_1_stream': 32768,\n",
    " 'fn_Cos_3__in_0_stream': 32768,\n",
    " 'fn_Mul_13__in_1_stream': 32768,\n",
    " 'fn_Cos_7__in_0_stream': 32768})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at 2023-04-10 01:06:22.736887\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(f\"Starting at {datetime.now()!s}\")\n",
    "print(\"Latency:\", best_so_far.get_latency(), \"cycles\")\n",
    "print(f\"Finished at {datetime.now()!s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(n, np.prod(x[\"shape\"])) for n, x in G_data.nodes.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying depth 0... cycle found\n",
      "Trying depth 1024... cycle found\n",
      "Trying depth 2048... cycle found\n",
      "Trying depth 3072... cycle found\n",
      "Trying depth 4096... cycle found\n",
      "Trying depth 5120... cycle found\n",
      "Trying depth 6144... cycle found\n",
      "Trying depth 7168... cycle found\n",
      "Trying depth 8192... cycle found\n",
      "Trying depth 9216... cycle found\n",
      "Trying depth 10240... cycle found\n",
      "Trying depth 11264... cycle found\n",
      "Trying depth 12288... cycle found\n",
      "Trying depth 13312... cycle found\n",
      "Trying depth 14336... cycle found\n",
      "Trying depth 15360... cycle found\n",
      "Trying depth 16384... cycle found\n",
      "Trying depth 17408... cycle found\n",
      "Trying depth 18432... cycle found\n",
      "Trying depth 19456... cycle found\n",
      "Trying depth 20480... cycle found\n",
      "Trying depth 21504... cycle found\n",
      "Trying depth 22528... no cycle found\n"
     ]
    }
   ],
   "source": [
    "# for i in range(0, 1024 * 1024, 1024):\n",
    "#     print(f\"Trying depth {i}... \", end=\"\")\n",
    "#     if dfg.with_depths({stream.name: i for stream in dfg.nodes.streams}).has_cycle():\n",
    "#         print(\"cycle found\")\n",
    "#     else:\n",
    "#         print(\"no cycle found\")\n",
    "#         break\n",
    "\n",
    "# Results: no cycle found at depth 22528"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying depth 0... cycle found\n",
      "Trying depth 1024... cycle found\n",
      "Trying depth 2048... cycle found\n",
      "Trying depth 3072... cycle found\n",
      "Trying depth 4096... cycle found\n",
      "Trying depth 5120... cycle found\n",
      "Trying depth 6144... cycle found\n",
      "Trying depth 7168... cycle found\n",
      "Trying depth 8192... cycle found\n",
      "Trying depth 9216... cycle found\n",
      "Trying depth 10240... cycle found\n",
      "Trying depth 11264... cycle found\n",
      "Trying depth 12288... cycle found\n",
      "Trying depth 13312... cycle found\n",
      "Trying depth 14336... cycle found\n",
      "Trying depth 15360... cycle found\n",
      "Trying depth 16384... cycle found\n",
      "Trying depth 17408... cycle found\n",
      "Trying depth 18432... cycle found\n",
      "Trying depth 19456... cycle found\n",
      "Trying depth 20480... cycle found\n",
      "Trying depth 21504... cycle found\n",
      "Trying depth 22528... cycle found\n",
      "Trying depth 23552... cycle found\n",
      "Trying depth 24576... cycle found\n",
      "Trying depth 25600... cycle found\n",
      "Trying depth 26624... cycle found\n",
      "Trying depth 27648... cycle found\n",
      "Trying depth 28672... cycle found\n",
      "Trying depth 29696... cycle found\n",
      "Trying depth 30720... cycle found\n",
      "Trying depth 31744... cycle found\n",
      "Trying depth 32768... no cycle found\n"
     ]
    }
   ],
   "source": [
    "# for i in range(0, 1024 * 1024, 1024):\n",
    "#     print(f\"Trying depth {i}... \", end=\"\")\n",
    "#     if dfg.with_depths({stream.name: (i if not stream.name.endswith((\"__out_stream\", \"__temp_stream\")) else 2) for stream in dfg.nodes.streams}).has_cycle():\n",
    "#         print(\"cycle found\")\n",
    "#     else:\n",
    "#         print(\"no cycle found\")\n",
    "#         break\n",
    "\n",
    "# Results: no cycle found at depth 32768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying depth 32765... cycle found\n",
      "Trying depth 32766... no cycle found\n"
     ]
    }
   ],
   "source": [
    "# for i in range(32765, 32767):\n",
    "#     print(f\"Trying depth {i}... \", end=\"\")\n",
    "#     if dfg.with_depths({stream.name: (i if not stream.name.endswith((\"__out_stream\", \"__temp_stream\")) else 2) for stream in dfg.nodes.streams}).has_cycle():\n",
    "#         print(\"cycle found\")\n",
    "#     else:\n",
    "#         print(\"no cycle found\")\n",
    "#         break\n",
    "\n",
    "# Results:\n",
    "# Trying depth 32765... cycle found\n",
    "# Trying depth 32766... no cycle found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {stream.name: (i if not stream.name.endswith((\"__out_stream\", \"__temp_stream\")) else 2) for stream in dfg.nodes.streams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying depth 262912... cycle found\n",
      "Trying depth 393856... cycle found\n",
      "Trying depth 459328... cycle found\n",
      "Trying depth 492064... cycle found\n",
      "Trying depth 508432... cycle found\n",
      "Trying depth 516616... cycle found\n",
      "Trying depth 520708... cycle found\n",
      "Trying depth 522754... cycle found\n",
      "Trying depth 523777... cycle found\n",
      "Trying depth 524289... no cycle found\n",
      "Trying depth 524033... cycle found\n",
      "Trying depth 524161... cycle found\n",
      "Trying depth 524225... cycle found\n",
      "Trying depth 524257... cycle found\n",
      "Trying depth 524273... cycle found\n",
      "Trying depth 524281... cycle found\n",
      "Trying depth 524285... cycle found\n",
      "Trying depth 524287... no cycle found\n",
      "Trying depth 524286... no cycle found\n"
     ]
    }
   ],
   "source": [
    "# lo = 1024\n",
    "# hi = 524800\n",
    "# while hi > lo:\n",
    "#     mid = (hi + lo) // 2\n",
    "#     print(f\"Trying depth {mid}..\", end=\"\")\n",
    "#     dfg_modified = dfg.with_depths({stream.name: (mid if not stream.name.endswith((\"__out_stream\", \"__temp_stream\")) else 2) for stream in dfg.nodes.streams})\n",
    "#     print(\". \", end=\"\")\n",
    "#     if dfg_modified.has_cycle():\n",
    "#         print(\"cycle found\")\n",
    "#         lo = mid + 1\n",
    "#     else:\n",
    "#         print(\"no cycle found\")\n",
    "#         hi = mid\n",
    "\n",
    "# Results (on bs=4096):\n",
    "# Trying depth 262912... cycle found\n",
    "# Trying depth 393856... cycle found\n",
    "# Trying depth 459328... cycle found\n",
    "# Trying depth 492064... cycle found\n",
    "# Trying depth 508432... cycle found\n",
    "# Trying depth 516616... cycle found\n",
    "# Trying depth 520708... cycle found\n",
    "# Trying depth 522754... cycle found\n",
    "# Trying depth 523777... cycle found\n",
    "# Trying depth 524289... no cycle found\n",
    "# Trying depth 524033... cycle found\n",
    "# Trying depth 524161... cycle found\n",
    "# Trying depth 524225... cycle found\n",
    "# Trying depth 524257... cycle found\n",
    "# Trying depth 524273... cycle found\n",
    "# Trying depth 524281... cycle found\n",
    "# Trying depth 524285... cycle found\n",
    "# Trying depth 524287... no cycle found\n",
    "# Trying depth 524286... no cycle found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of src.analysis.latency.engine failed: Traceback (most recent call last):\n",
      "  File \"/usr/scratch/rsarkar30/miniconda/envs/INSPNet/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 257, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/usr/scratch/rsarkar30/miniconda/envs/INSPNet/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 455, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/usr/scratch/rsarkar30/miniconda/envs/INSPNet/lib/python3.10/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1017, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 947, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/export/hdd/scratch/rsarkar30/INR/inr-dsp/src/analysis/latency/engine.py\", line 122\n",
      "    num_components, labels: Tuple[int, npt.NDArray[np.intp]] = connected_components(self.graph, connection=\"strong\")\n",
      "    ^^^^^^^^^^^^^^\n",
      "SyntaxError: only single target (not tuple) can be annotated\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse.csgraph import connected_components\n",
    "# dfg2 = dfg.with_depths({stream.name: 2 for stream in dfg.nodes.streams})\n",
    "cc = connected_components(dfg.graph, connection=\"strong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29776], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_component_labels, component_label_counts = np.unique(component_labels, return_counts=True)\n",
    "cycles = unique_component_labels[component_label_counts > 1]\n",
    "cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids, = np.nonzero(component_labels == cycles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18873402"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(component_labels == cycles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.analysis.latency.engine import DFGNodeTable, DFG_ROOT, DFGNode\n",
    "# from typing import List, Union\n",
    "# import numpy.typing as npt\n",
    "# def lookup_many_reverse(self: DFGNodeTable, node_ids: npt.NDArray[np.int64]):\n",
    "#     nodes: List[Union[DFGNode, None]] = [None] * len(node_ids)\n",
    "#     root_idxs, = np.nonzero(node_ids == 0)\n",
    "#     for i in root_idxs:\n",
    "#         nodes[i] = DFG_ROOT\n",
    "#     for stream, lo in self.forward_table.items():\n",
    "#         hi = lo + stream.num_writes * 2\n",
    "#         mask = (node_ids >= lo) & (node_ids < hi)\n",
    "#         idxs, = np.nonzero(mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "059eb268473ffbf501937b421bd02a0288b0568cab5961bc988a7fb0ad849296"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
